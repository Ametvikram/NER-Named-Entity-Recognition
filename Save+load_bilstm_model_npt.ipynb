{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model without CRF\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "data = pd.read_csv('/home/sharath/postags_v1.csv', encoding='latin1', sep = ',')\n",
    "data = data.fillna(method=\"ffill\")\n",
    "words = list(set(data[\"Word\"].values))\n",
    "words.append(\"ENDPAD\")\n",
    "n_words = len(words)\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "n_tags = len(tags)\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "sent = getter.get_next()\n",
    "sentences = getter.sentences\n",
    "max_len = 50\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words - 1)\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "#from keras.utils import to_categorical\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=n_words, output_dim=50, input_length=max_len)(input)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48 samples, validate on 6 samples\n",
      "Epoch 1/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0425 - acc: 0.9925 - val_loss: 0.2413 - val_acc: 0.9433\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0403 - acc: 0.9942 - val_loss: 0.2285 - val_acc: 0.9533\n",
      "Epoch 3/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0374 - acc: 0.9929 - val_loss: 0.2340 - val_acc: 0.9567\n",
      "Epoch 4/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0368 - acc: 0.9946 - val_loss: 0.2291 - val_acc: 0.9533\n",
      "Epoch 5/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0338 - acc: 0.9962 - val_loss: 0.2304 - val_acc: 0.9533\n",
      "Epoch 6/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0326 - acc: 0.9967 - val_loss: 0.2277 - val_acc: 0.9567\n",
      "Epoch 7/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0307 - acc: 0.9963 - val_loss: 0.2315 - val_acc: 0.9533\n",
      "Epoch 8/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0303 - acc: 0.9958 - val_loss: 0.2259 - val_acc: 0.9567\n",
      "Epoch 9/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0274 - acc: 0.9971 - val_loss: 0.2330 - val_acc: 0.9533\n",
      "Epoch 10/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0296 - acc: 0.9946 - val_loss: 0.2359 - val_acc: 0.9567\n",
      "Epoch 11/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0293 - acc: 0.9962 - val_loss: 0.2316 - val_acc: 0.9533\n",
      "Epoch 12/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0241 - acc: 0.9979 - val_loss: 0.2325 - val_acc: 0.9533\n",
      "Epoch 13/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0218 - acc: 0.9979 - val_loss: 0.2257 - val_acc: 0.9567\n",
      "Epoch 14/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0226 - acc: 0.9975 - val_loss: 0.2312 - val_acc: 0.9567\n",
      "Epoch 15/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0216 - acc: 0.9975 - val_loss: 0.2236 - val_acc: 0.9600\n",
      "Epoch 16/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0192 - acc: 0.9983 - val_loss: 0.2302 - val_acc: 0.9567\n",
      "Epoch 17/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0187 - acc: 0.9979 - val_loss: 0.2344 - val_acc: 0.9633\n",
      "Epoch 18/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0207 - acc: 0.9979 - val_loss: 0.2307 - val_acc: 0.9633\n",
      "Epoch 19/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0191 - acc: 0.9983 - val_loss: 0.2309 - val_acc: 0.9533\n",
      "Epoch 20/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0197 - acc: 0.9971 - val_loss: 0.2336 - val_acc: 0.9567\n",
      "Epoch 21/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0180 - acc: 0.9979 - val_loss: 0.2361 - val_acc: 0.9533\n",
      "Epoch 22/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0176 - acc: 0.9983 - val_loss: 0.2366 - val_acc: 0.9567\n",
      "Epoch 23/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0176 - acc: 0.9979 - val_loss: 0.2287 - val_acc: 0.9567\n",
      "Epoch 24/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0128 - acc: 0.9987 - val_loss: 0.2440 - val_acc: 0.9567\n",
      "Epoch 25/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0138 - acc: 0.9988 - val_loss: 0.2369 - val_acc: 0.9567\n",
      "Epoch 26/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0127 - acc: 0.9983 - val_loss: 0.2358 - val_acc: 0.9567\n",
      "Epoch 27/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0122 - acc: 0.9987 - val_loss: 0.2398 - val_acc: 0.9567\n",
      "Epoch 28/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0117 - acc: 0.9996 - val_loss: 0.2356 - val_acc: 0.9567\n",
      "Epoch 29/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0113 - acc: 0.9996 - val_loss: 0.2396 - val_acc: 0.9567\n",
      "Epoch 30/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0105 - acc: 0.9987 - val_loss: 0.2453 - val_acc: 0.9600\n",
      "Epoch 31/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0110 - acc: 0.9996 - val_loss: 0.2416 - val_acc: 0.9633\n",
      "Epoch 32/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.2382 - val_acc: 0.9567\n",
      "Epoch 33/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0086 - acc: 0.9996 - val_loss: 0.2488 - val_acc: 0.9633\n",
      "Epoch 34/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0082 - acc: 0.9987 - val_loss: 0.2421 - val_acc: 0.9600\n",
      "Epoch 35/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0101 - acc: 0.9983 - val_loss: 0.2365 - val_acc: 0.9633\n",
      "Epoch 36/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0118 - acc: 0.9983 - val_loss: 0.2364 - val_acc: 0.9633\n",
      "Epoch 37/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0080 - acc: 0.9992 - val_loss: 0.2408 - val_acc: 0.9633\n",
      "Epoch 38/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0085 - acc: 0.9987 - val_loss: 0.2457 - val_acc: 0.9633\n",
      "Epoch 39/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0081 - acc: 0.9992 - val_loss: 0.2362 - val_acc: 0.9567\n",
      "Epoch 40/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0071 - acc: 0.9996 - val_loss: 0.2440 - val_acc: 0.9633\n",
      "Epoch 41/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.2450 - val_acc: 0.9633\n",
      "Epoch 42/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.2463 - val_acc: 0.9600\n",
      "Epoch 43/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.2482 - val_acc: 0.9600\n",
      "Epoch 44/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.2384 - val_acc: 0.9633\n",
      "Epoch 45/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0056 - acc: 0.9996 - val_loss: 0.2549 - val_acc: 0.9600\n",
      "Epoch 46/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0058 - acc: 0.9996 - val_loss: 0.2595 - val_acc: 0.9600\n",
      "Epoch 47/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0074 - acc: 0.9987 - val_loss: 0.2503 - val_acc: 0.9633\n",
      "Epoch 48/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0045 - acc: 0.9996 - val_loss: 0.2447 - val_acc: 0.9600\n",
      "Epoch 49/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0045 - acc: 0.9996 - val_loss: 0.2435 - val_acc: 0.9633\n",
      "Epoch 50/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.2492 - val_acc: 0.9633\n",
      "Epoch 51/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.2529 - val_acc: 0.9600\n",
      "Epoch 52/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.2580 - val_acc: 0.9667\n",
      "Epoch 53/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0057 - acc: 0.9996 - val_loss: 0.2424 - val_acc: 0.9567\n",
      "Epoch 54/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0045 - acc: 0.9996 - val_loss: 0.2422 - val_acc: 0.9633\n",
      "Epoch 55/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.2464 - val_acc: 0.9600\n",
      "Epoch 56/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.2539 - val_acc: 0.9633\n",
      "Epoch 57/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.2500 - val_acc: 0.9633\n",
      "Epoch 58/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0044 - acc: 0.9992 - val_loss: 0.2520 - val_acc: 0.9633\n",
      "Epoch 59/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.2521 - val_acc: 0.9633\n",
      "Epoch 60/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0030 - acc: 0.9996 - val_loss: 0.2706 - val_acc: 0.9567\n",
      "Epoch 61/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0125 - acc: 0.9975 - val_loss: 0.2482 - val_acc: 0.9633\n",
      "Epoch 62/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.2522 - val_acc: 0.9633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.2528 - val_acc: 0.9667\n",
      "Epoch 64/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.2567 - val_acc: 0.9600\n",
      "Epoch 65/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0029 - acc: 0.9996 - val_loss: 0.2572 - val_acc: 0.9633\n",
      "Epoch 66/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.2585 - val_acc: 0.9633\n",
      "Epoch 67/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.2554 - val_acc: 0.9600\n",
      "Epoch 68/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.2539 - val_acc: 0.9667\n",
      "Epoch 69/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.2506 - val_acc: 0.9633\n",
      "Epoch 70/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.2574 - val_acc: 0.9633\n",
      "Epoch 71/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.2592 - val_acc: 0.9667\n",
      "Epoch 72/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2619 - val_acc: 0.9667\n",
      "Epoch 73/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2641 - val_acc: 0.9633\n",
      "Epoch 74/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.2573 - val_acc: 0.9667\n",
      "Epoch 75/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2619 - val_acc: 0.9633\n",
      "Epoch 76/100\n",
      "48/48 [==============================] - 0s 3ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.2629 - val_acc: 0.9667\n",
      "Epoch 77/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2635 - val_acc: 0.9633\n",
      "Epoch 78/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2614 - val_acc: 0.9667\n",
      "Epoch 79/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2552 - val_acc: 0.9667\n",
      "Epoch 80/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.2620 - val_acc: 0.9633\n",
      "Epoch 81/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2796 - val_acc: 0.9667\n",
      "Epoch 82/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.2658 - val_acc: 0.9633\n",
      "Epoch 83/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2721 - val_acc: 0.9667\n",
      "Epoch 84/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0022 - acc: 0.9996 - val_loss: 0.2719 - val_acc: 0.9667\n",
      "Epoch 85/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2586 - val_acc: 0.9667\n",
      "Epoch 86/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.2644 - val_acc: 0.9700\n",
      "Epoch 87/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 8.4903e-04 - acc: 1.0000 - val_loss: 0.2673 - val_acc: 0.9700\n",
      "Epoch 88/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 6.8385e-04 - acc: 1.0000 - val_loss: 0.2698 - val_acc: 0.9667\n",
      "Epoch 89/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 8.8324e-04 - acc: 1.0000 - val_loss: 0.2721 - val_acc: 0.9700\n",
      "Epoch 90/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 7.0830e-04 - acc: 1.0000 - val_loss: 0.2757 - val_acc: 0.9667\n",
      "Epoch 91/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 7.1568e-04 - acc: 1.0000 - val_loss: 0.2736 - val_acc: 0.9667\n",
      "Epoch 92/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 6.9119e-04 - acc: 1.0000 - val_loss: 0.2721 - val_acc: 0.9633\n",
      "Epoch 93/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 6.6900e-04 - acc: 1.0000 - val_loss: 0.2745 - val_acc: 0.9667\n",
      "Epoch 94/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 6.2851e-04 - acc: 1.0000 - val_loss: 0.2778 - val_acc: 0.9700\n",
      "Epoch 95/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 6.8017e-04 - acc: 1.0000 - val_loss: 0.2710 - val_acc: 0.9700\n",
      "Epoch 96/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 6.7115e-04 - acc: 1.0000 - val_loss: 0.2659 - val_acc: 0.9667\n",
      "Epoch 97/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 5.2954e-04 - acc: 1.0000 - val_loss: 0.2774 - val_acc: 0.9700\n",
      "Epoch 98/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 5.7903e-04 - acc: 1.0000 - val_loss: 0.2714 - val_acc: 0.9667\n",
      "Epoch 99/100\n",
      "48/48 [==============================] - 0s 5ms/step - loss: 8.7568e-04 - acc: 1.0000 - val_loss: 0.2786 - val_acc: 0.9667\n",
      "Epoch 100/100\n",
      "48/48 [==============================] - 0s 4ms/step - loss: 7.5707e-04 - acc: 1.0000 - val_loss: 0.2653 - val_acc: 0.9667\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=100, validation_split=0.1, verbose=1)\n",
    "\n",
    "#model.save('mmmodel.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "#modelkk = load_model('mmmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"An individual wiping command should be implemented by setting the level of WW and INT-line from HIGH to LOW for the parameterized time p_t_einzelwischhub body control module\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in test_sentence]],\n",
    "                            padding=\"post\", value=0, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newpredict(tt):\n",
    "    result = []\n",
    "    x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in tt]],\n",
    "                            padding=\"post\", value=0, maxlen=max_len)\n",
    "    p = model.predict(np.array([x_test_sent[0]]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    for w, pred in zip(tt, p[0]):\n",
    "        result.append((w, tags[pred]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('An', 'Action'),\n",
       " ('individual', 'Action'),\n",
       " ('wiping', 'Action'),\n",
       " ('command', 'O'),\n",
       " ('should', 'Action'),\n",
       " ('be', 'O'),\n",
       " ('implemented', 'O'),\n",
       " ('by', 'O'),\n",
       " ('setting', 'O'),\n",
       " ('the', 'O'),\n",
       " ('level', 'O'),\n",
       " ('of', 'O'),\n",
       " ('WW', 'system_signal'),\n",
       " ('and', 'O'),\n",
       " ('INT-line', 'Action'),\n",
       " ('from', 'O'),\n",
       " ('HIGH', 'Action'),\n",
       " ('to', 'O'),\n",
       " ('LOW', 'Action'),\n",
       " ('for', 'O'),\n",
       " ('the', 'O'),\n",
       " ('parameterized', 'Action'),\n",
       " ('time', 'Action'),\n",
       " ('p_t_einzelwischhub', 'Action'),\n",
       " ('body', 'system component'),\n",
       " ('control', 'system component'),\n",
       " ('module', 'system component')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newpredict(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[('The', 'O'),\n",
    " ('system', 'Network_Component'),\n",
    " ('should', 'O'),\n",
    " ('do', 'O'),\n",
    " ('the', 'O'),\n",
    " ('validation', 'Action'),\n",
    " ('for', 'O'),\n",
    " ('confirm', 'Action'),\n",
    " ('password', 'Attribute'),\n",
    " ('text', 'O'),\n",
    " ('on', 'O'),\n",
    " ('submit', 'O'),\n",
    " ('of', 'O'),\n",
    " ('the', 'O'),\n",
    " ('form', 'O')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 1ms/step\n",
      "acc: 99.63%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_tr, np.array(y_tr), verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"/home/sharath/Documents/savedweights_Bilstm/modelpp6.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"/home/sharath/Documents/savedweights_Bilstm/modelpp6.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('/home/sharath/Documents/savedweights_Bilstm/modelpp6.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"/home/sharath/Documents/savedweights_Bilstm/modelpp6.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sharath'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 1s 11ms/step\n",
      "acc: 99.63%\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate(X_tr, np.array(y_tr), verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save\n",
    "np.save('/home/sharath/Documents/savedweights_Bilstm/word2idx.npy', word2idx) \n",
    "\n",
    "# Load\n",
    "#word2idx = np.load('C:\\\\Users\\\\sharath\\\\Desktop\\\\exp\\\\word2idx.npy').item()\n",
    "\n",
    "\n",
    "# Save\n",
    "np.save('/home/sharath/Documents/savedweights_Bilstm/tag2idx.npy', tag2idx) \n",
    "\n",
    "# Load\n",
    "#tag2idx = np.load('C:\\\\Users\\\\sharath\\\\Desktop\\\\exp\\\\tag2idx.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstmfinal(test_sentence):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from keras.models import model_from_json\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.models import Model, Input\n",
    "    from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from keras.utils import to_categorical\n",
    "    import numpy\n",
    "    numpy.random.seed(7)\n",
    "    word2idx = np.load('/home/sharath/Documents/savedweights_Bilstm/word2idx.npy').item()\n",
    "    tag2idx = np.load('/home/sharath/Documents/savedweights_Bilstm/tag2idx.npy').item()\n",
    "    # load json and create model\n",
    "    json_file = open('/home/sharath/Documents/savedweights_Bilstm/modelpp6.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"/home/sharath/Documents/savedweights_Bilstm/modelpp6.h5\")\n",
    "    loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    test_sentence = test_sentence.split(\" \")\n",
    "    x_test_sent = pad_sequences(sequences=[[word2idx.get(w, 0) for w in test_sentence]],\n",
    "                            padding=\"post\", value=0, maxlen=50)\n",
    "    p = loaded_model.predict(np.array([x_test_sent[0]]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    pls = []\n",
    "    p = loaded_model.predict(np.array([x_test_sent[0]]))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    for w, pred in zip(test_sentence, p[0]):\n",
    "        pls.append(pred)\n",
    "    revsubs = { v:k for k,v in tag2idx.items()}\n",
    "    entit = [revsubs.get(item,item) for item in pls]\n",
    "    final = list(zip(test_sentence,entit))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"The system shall allow a registered visitor to load a custom filter.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'O'),\n",
       " ('architecture', 'O'),\n",
       " ('of', 'O'),\n",
       " ('Hatch', 'auxillary system'),\n",
       " ('washer', 'auxillary system'),\n",
       " ('consists', 'O'),\n",
       " ('of', 'O'),\n",
       " ('a', 'O'),\n",
       " ('gateway', 'system component'),\n",
       " ('connection', 'system component'),\n",
       " ('to', 'system component'),\n",
       " ('Controller', 'system component'),\n",
       " ('Area', 'system component'),\n",
       " ('Network', 'system component'),\n",
       " ('signal', 'system_signal'),\n",
       " ('BUS', 'system_signal'),\n",
       " ('and', 'O'),\n",
       " ('a', 'O'),\n",
       " ('combination', 'O'),\n",
       " ('BUS', 'system_signal')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstmfinal(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def lstm_model():\n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "#     from keras.models import load_model\n",
    "\n",
    "\n",
    "#     data = pd.read_csv('C:\\\\users\\\\sharath\\\\postags_v1.csv', encoding='latin1', sep = ',')\n",
    "#     data = data.fillna(method=\"ffill\")\n",
    "#     words = list(set(data[\"Word\"].values))\n",
    "#     words.append(\"ENDPAD\")\n",
    "#     n_words = len(words)\n",
    "#     tags = list(set(data[\"Tag\"].values))\n",
    "#     n_tags = len(tags)\n",
    "\n",
    "#     class SentenceGetter(object):\n",
    "\n",
    "#         def __init__(self, data):\n",
    "#             self.n_sent = 1\n",
    "#             self.data = data\n",
    "#             self.empty = False\n",
    "#             agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "#                                                                s[\"POS\"].values.tolist(),\n",
    "#                                                                s[\"Tag\"].values.tolist())]\n",
    "#             self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "#             self.sentences = [s for s in self.grouped]\n",
    "\n",
    "#         def get_next(self):\n",
    "#             try:\n",
    "#                 s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "#                 self.n_sent += 1\n",
    "#                 return s\n",
    "#             except:\n",
    "#                 return None\n",
    "\n",
    "#     getter = SentenceGetter(data)\n",
    "#     sent = getter.get_next()\n",
    "#     sentences = getter.sentences\n",
    "#     max_len = 50\n",
    "#     word2idx = {w: i for i, w in enumerate(words)}\n",
    "#     tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "#     from keras.preprocessing.sequence import pad_sequences\n",
    "#     X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "#     X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words - 1)\n",
    "#     y = [[tag2idx[w[2]] for w in s] for s in sentences]\n",
    "#     y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "#     from keras.utils import to_categorical\n",
    "#     y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1)\n",
    "#     from keras.models import Model, Input\n",
    "#     from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "#     input = Input(shape=(max_len,))\n",
    "#     model = Embedding(input_dim=n_words, output_dim=50, input_length=max_len)(input)\n",
    "#     model = Dropout(0.1)(model)\n",
    "#     model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    "#     out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n",
    "#     model = Model(input, out)\n",
    "#     model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#     history = model.fit(X_tr, np.array(y_tr), batch_size=32, epochs=10, validation_split=0.1, verbose=1)\n",
    "#     history.model.save('m3odel.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#     del model  # deletes the existing model\n",
    "#     # returns a compiled model\n",
    "#     # identical to the previous one\n",
    "#     modelkk = load_model('m3odel.h5')\n",
    "#     return modelkk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x1ba7dc0d0f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = \"The system shall allow the data manager to search occurrence resources by keywords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = \"The system shall allow the data manager to edit the metadata of an institution resource\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
